# Deep Learning Specialization on Coursera (offered by deeplearning.ai)

Programming assignments and quizzes from all courses in the Coursera [Deep Learning specialization](https://www.coursera.org/specializations/deep-learning) offered by `deeplearning.ai`.

Instructor: [Andrew Ng](http://www.andrewng.org/)

## Credits
This repo contains my work for this specialization. The code base, quiz questions and diagrams are taken from the [Deep Learning Specialization on Coursera](https://www.coursera.org/specializations/deep-learning), unless specified otherwise.

## Programming Assignments
This page will describe in short about each project, the core learning and some tips.

### Course 1: Neural Networks and Deep Learning

  - [Week 2 - PA 1 - Python Basics with Numpy](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Python%20Basics%20with%20Numpy/Python_Basics_With_Numpy_v3a.ipynb)
  - [Week 2 - PA 2 - Logistic Regression with a Neural Network mindset](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/Logistic_Regression_with_a_Neural_Network_mindset_v6a.ipynb)
  - [Week 3 - PA 3 - Planar data classification with one hidden layer](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Planar%20data%20classification%20with%20one%20hidden%20layer/Planar_data_classification_with_onehidden_layer_v6c.ipynb)
  - [Week 4 - PA 4 - Building your Deep Neural Network: Step by Step](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step/Building_your_Deep_Neural_Network_Step_by_Step_v8a.ipynb)
  - [Week 4 - PA 5 - Deep Neural Network for Image Classification: Application](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Deep%20Neural%20Network%20Application_%20Image%20Classification/Deep%20Neural%20Network%20-%20Application%20v8.ipynb)

### Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

  - [Week 1 - PA 1 - Initialization](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Initialization/Initialization.ipynb)
  - [Week 1 - PA 2 - Regularization](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Regularization/Regularization_v2a.ipynb)
  - [Week 1 - PA 3 - Gradient Checking](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Gradient%20Checking/Gradient%20Checking%20v1.ipynb)
  - [Week 2 - PA 4 - Optimization Methods](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Optimization_methods_v1b.ipynb)
  - [Week 3 - PA 5 - TensorFlow Tutorial](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/TensorFlow_Tutorial_v3b.ipynb)

### Course 3: Structuring Machine Learning Projects

  - There are no PAs for this course. But this course comes with very interesting case study quizzes (below).
  
### Course 4: Convolutional Neural Networks

  - [Week 1 - PA 1 - Convolutional Model: step by step](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Convolution_model_Step_by_Step_v2a.ipynb)
  - [Week 1 - PA 2 - Convolutional Neural Networks: Application](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Convolution_model_Application_v1a.ipynb)
  - [Week 2 - PA 1 - Keras - Tutorial - Happy House](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/KerasTutorial/Keras%20-%20Tutorial%20-%20Happy%20House%20v2.ipynb)
  - [Week 2 - PA 2 - Residual Networks](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/ResNets/Residual_Networks_v2a.ipynb)
  - [Week 3 - PA 1 - Car detection with YOLO for Autonomous Driving](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Car%20detection%20for%20Autonomous%20Driving/Autonomous_driving_application_Car_detection_v3a.ipynb)
  - [Week 4 - PA 1 - Art Generation with Neural Style Transfer](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Neural%20Style%20Transfer/Art_Generation_with_Neural_Style_Transfer_v3a.ipynb)    
  - [Week 4 - PA 2 - Face Recognition](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Face%20Recognition/Face_Recognition_v3a.ipynb)
  
### Course 5: Sequence Models

  - [Week 1 - PA 1 - Building a Recurrent Neural Network - Step by Step](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Building%20a%20Recurrent%20Neural%20Network%20-%20Step%20by%20Step/Building_a_Recurrent_Neural_Network_Step_by_Step_v3a.ipynb)
      - This project is all about defining your own RELU,LSTM units(cells) , stitch these unit together to make RNN network topology (depending upon Tx) and finally doing forward and backward propagation passes. The math around backpropagation is hairy. The project is done using numpy library and doing all these operations from scratch. In practise you are better off using a library like tensorflow/keras. Main takeaway from this project. 
        1. For the runtime effciency reasons we process the training example in batches so the tensor which is processed is (one-hot-encoded-input-size,batch-size,time-step-size) == (Na,m,Tx). Similarly the hidden state tensor is (na,m,TX) output state tensor is (ny,m,Ty)
        2. All parameters in RNN cells are shared. you can assume that there is just one cell which is consuming different input at different time-step. For those who are familiar with circuits it is kind of a FSM mealy machine. 
        3. This project deal with sinlge laryer RNN network. In parctise RNN are not made too deep (>100 layers etc) as the general neural network are done. This is due to the fact that RNN also have a time-step dimention (Tx). Runtime of this 1 layer RNN will depend on how large the Tx and how large state "a" (100 is not uncommon). 
        4. RNN based on standard RNN cell will suffer from vanishing gradients (which makes training very hard for large Tx), thus it is not uncommon to use LSTM cell (which have a longterm memory components) in practise.  
  - [Week 1 - PA 2 - Dinosaur Land -- Character-level Language Modeling](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Dinosaur%20Island%20--%20Character-level%20language%20model/Dinosaurus_Island_Character_level_language_model_final_v3a.ipynb)
    - Character level language model is classic application of RNN. It does two task 1) build a RNN (character based model) 2) Generate sequence (in this case Dinasaur names) based on character language model learned by doing weighted random sampling at each step of output and feeding it to input in next step.
      -  Character level RNN  language model: This project use standard RNN cells to make a RNN network. It leverage same routine which is make in the Week 1 PA #1. Tx depends on how long is your input sequence (in this case a dinosour name). Since the input (dinasour name) in this case vary from one example to other this project does not use batches during training process, making it slower to train.In practise input with same length can be club together to make batches or for character level model you can always make a data by having a sliding window the input (with some stride) and generate a output. Another way to handle varying input size which is quiet popular is to append 0 in inputs which are short. Most programming framework does not allow you to have varying input size (because such training data can't be vectorized easily). It is quiet common to 0 pad (append) the input during prepocessing time.   
      - In this project we have total ~1500 dinasour names. Input X = dinasour name, Y = Name_Shifted_One_Step_to_left + new_line
      - Some parameters used in the train process vocab_size = 27 , n_a = 50 ( hidden state size)
      - Random sampling in output to genreate new sequences: We start with zero vector and use the model generated above. For each time-step we randomly choose and output np.random.choice(). This ouptut is fed to the next cell as input. We stop the sampling when the size hits 50 or a EOW (newline) is encountered.
      -  Gradient clipping np.clip() is used to clip the gradient if they go beyond (-5,+5). This solve the exploding gradient problem. Since there are not too many character in the dinasour name the Tx is not so large to have a vanishing gradient problem. 
      -  Model parameters are global and they are shared across multiple iterations of training (using cache passed along the iterations)
    
  - [Week 1 - PA 3 - Jazz improvisation with LSTM](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Jazz%20improvisation%20with%20LSTM/Improvise_a_Jazz_Solo_with_an_LSTM_Network_v3a.ipynb) 
    - This project generate music. This is build using Keras. It build music model first and then based on random sampling it generate a new music sequence. Built music is then postprocessed to make it sound better. For traiing there are 60 music clips used each of lengh 30 (2 values for each second of music). Total unique values for note (value) is 90 (dictionary size). Hence in terms of Keras the input tensor is (60(m),30(Tx),90(Nx)). The Y is (Ty,m,90) where Ty is shifted by 1 to left.
    - Keras tips: Since the weights are shared across the time-steps of RNN, the global layer object need to be defined which holds the weights. these are instance of Reshape(), LSTM(), Dense(). These common instances are used in the  model definition part. You can think of model definition as stitching of Tx LSTM cells together. Defining model is just defining architecure of the model not executing it.
      - Once model is define we instantiate it (make a object of model). model.summary() will give the summary of parameters of your model. We have ~45K parameters.
      - Once model object is instantiated we need to compile it (model_obj.compile()). During compile you need to specify the type of optimization engine (Adam etc) to be used and what defines a loss.
      - Once the model is compile you can execute it by using model_object.fit()
      - Training is done using the shifted input trick. The output is just the input shifted by 1 (towards left) and one hot encoded.
  - [Week 2 - PA 1 - Word Vector Representation and Debiasing](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Word%20Vector%20Representation/Operations_on_word_vectors_v2a.ipynb)  
    - This project is all about word embedding. Word embedding (encoding) is a powerful method to encode text input (words) to capture relation among the words. In practise most programming use pre-trained word embedding. Using word-embedding is almost more superior to One-hot encoding.
      - Once you have a word embedding encoding, you can use it to feed your LSTM network. In this case the learning is much better than feeding in the one hot encoding. This project use pretained GLove model with 50 dimentions.
      - To determine similarity between two words (represented/encoded using Glove/Word2vec) you can you eculidian distance or cosine similarity. like u.v/||u||.||v|| where ||v|| represent L2 norm = np.sqrt(np.sum(v*v)). This is used for word analogy task in this project.    
    - This project also debias the word embedding  by 1) Neutralizing bias for "non-gender words" 2) Equalization algorithm for "gender specific words."
  - [Week 2 - PA 2 - Emojify!](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Emojify/Emojify_v2a.ipynb)  
    - This project create a emoji for text. The biggest power comes for using encoding of words by Glove/Word2vec method and feed it to an LSTM network to train. This is sort of transfer learning as the Glove/Word2vec is train in large dataset
      - For the taining we just use ~150 examples. Even though this looks very small dataset the results are surprising good > 90% accuracy.Thanks to the power of word embedding and transfer learning.
      -  This project covers two method of doing the task. One is Just taking the embedding and averaging out the vectors and feeding it to softmax and minimizing the loss function. This is train the weigths of softmax(w.avg(i) +b) namely W,b. This is used later for prediction. This yield >80% accuracy in test set. This method does not take into account the relative position of words in a sentense so it is rather premitive.
     - A more powerful version of this is based on LSTM (RNN) : Words --> embedding --> LSTM --> Dropout --> LSTM --> Dropout --> Softmax.
       - Keras (and other frameworks) will force you to 0 pad input which are shorter. Keras LSTM (RNN) Tx size have to be fixed.
       - We use Input(), LSTM(),Dropout(),Dense(),Activation() layers / objects of keras. to define the model first then compile (model_obj.compile()) and train it model_obj.fit() and model.evaluate() for prediction purposes.   
  - [Week 3 - PA 1 - Neural Machine Translation with Attention](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Machine%20Translation/Neural_machine_translation_with_attention_v4a.ipynb)  
  - [Week 3 - PA 2 - Trigger Word Detection](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Trigger%20word%20detection/Trigger_word_detection_v1a.ipynb)   

## Quiz Solutions

### Course 1: Neural Networks and Deep Learning

  - Week 1 Quiz - Introduction to deep learning: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%201/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%201/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.pdf)
  - Week 2 Quiz - Neural Network Basics: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Week%202%20Quiz%20-%20Neural%20Network%20Basics.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Week%202%20Quiz%20-%20Neural%20Network%20Basics.pdf)
  - Week 3 Quiz - Shallow Neural Networks: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Week%203%20Quiz%20-%20Shallow%20Neural%20Networks.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Week%203%20Quiz%20-%20Shallow%20Neural%20Networks.pdf)
  - Week 4 Quiz - Key concepts on Deep Neural Networks: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Week%204%20Quiz%20-%20Key%20concepts%20on%20Deep%20Neural%20Networks.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Week%204%20Quiz%20-%20Key%20concepts%20on%20Deep%20Neural%20Networks.pdf)

### Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

  - Week 1 Quiz - Practical aspects of deep learning: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Week%201%20Quiz%20-%20Practical%20aspects%20of%20deep%20learning.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Week%201%20Quiz%20-%20Practical%20aspects%20of%20deep%20learning.pdf)
  - Week 2 Quiz - Optimization algorithms: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Week%202%20Quiz%20-%20Optimization%20algorithms.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Week%202%20Quiz%20-%20Optimization%20algorithms.pdf)
  - Week 3 Quiz - Hyperparameter tuning, Batch Normalization, Programming Frameworks: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Week%203%20Quiz%20-%20Hyperparameter%20tuning%2C%20Batch%20Normalization%2C%20Programming%20Frameworks.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Week%203%20Quiz%20-%20Hyperparameter%20tuning%2C%20Batch%20Normalization%2C%20Programming%20Frameworks.pdf)
  
### Course 3: Structuring Machine Learning Projects

  - Week 1 Quiz - Bird recognition in the city of Peacetopia (case study): [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%201%20Quiz%20-%20Bird%20recognition%20in%20the%20city%20of%20Peacetopia%20(case%20study).md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%201%20Quiz%20-%20Bird%20recognition%20in%20the%20city%20of%20Peacetopia%20(case%20study).pdf)
  - Week 2 Quiz - Autonomous driving (case study): [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%202%20Quiz%20-%20Autonomous%20driving%20(case%20study).md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%202%20Quiz%20-%20Autonomous%20driving%20(case%20study).pdf)

### Course 4: Convolutional Neural Networks

  - Week 1 Quiz - The basics of ConvNets: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Week%201%20Quiz%20-%20The%20basics%20of%20ConvNets.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Week%201%20Quiz%20-%20The%20basics%20of%20ConvNets.pdf)
  - Week 2 Quiz - Deep convolutional models: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Week%202%20Quiz%20-%20Deep%20convolutional%20models.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Week%202%20Quiz%20-%20Deep%20convolutional%20models.pdf)
  - Week 3 Quiz - Detection algorithms: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Week%203%20Quiz%20-%20Detection%20algorithms.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Week%203%20Quiz%20-%20Detection%20algorithms.pdf)
  - Week 4 Quiz - Special applications: Face recognition & Neural style transfer: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Week%204%20Quiz%20-%20Special%20applications%20Face%20Recognition%20and%20Neural%20Style%20Transfer.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Week%204%20Quiz%20-%20Special%20applications%20Face%20Recognition%20and%20Neural%20Style%20Transfer.pdf)

### Course 5: Sequence Models

  - Week 1 Quiz - Recurrent Neural Networks: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Week%201%20Quiz%20-%20Recurrent%20Neural%20Networks.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Week%201%20Quiz%20-%20Recurrent%20Neural%20Networks.pdf)
  - Week 2 Quiz - Natural Language Processing & Word Embeddings: [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Week%202%20Quiz%20-%20Natural%20Language%20Processing%20%26%20Word%20Embeddings.pdf)
  - Week 3 Quiz - Sequence models & Attention mechanism: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Week%203%20Quiz%20-%20Sequence%20models%20%26%20Attention%20mechanisms.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Week%203%20Quiz%20-%20Sequence%20models%20%26%20Attention%20mechanisms.pdf)
